# NNN — Experiment Name

**Engine:** `src/engines/engine_file.py`
**Results:** `results/NNN-experiment-name.json`
**Date:** YYYY-MM-DD

## What I tried

(Describe the approach. What search technique? Which fields from the opinion
JSON? Any weighting, boosting, or filtering? If this builds on a previous
experiment, reference it: "Starting from 002, but adding...")

## Key numbers

| Metric   | Overall | Keyword | NatLang | FactPat |
|----------|---------|---------|---------|---------|
| MRR      |         |         |         |         |
| nDCG@5   |         |         |         |         |
| nDCG@10  |         |         |         |         |
| R@20     |         |         |         |         |

## Observations

(What worked? What didn't? Any queries that scored 0 on MRR — meaning the
engine completely missed the best answer? Patterns by topic or query type?
Anything surprising in the per-query results?)

## Next

(Based on what you learned, what's worth trying? This section is the most
valuable one — it's the thread that connects one experiment to the next.)
